{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dzOPshOZkUL"
      },
      "source": [
        "## Objective of the Test\n",
        "\n",
        "This test explores the effectiveness of using **historical returns vs. forecasted returns** in portfolio modeling and prediction. Additionally, it compares models that rely solely on **price history** with those that incorporate **alternative data** (macroeconomic indicators and Google Trends). The goal is to assess whether incorporating forecasted returns and external signals provides a measurable advantage over traditional methods based only on past prices.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIa6TqRHZwWR"
      },
      "source": [
        "Time interval is chosen due to constrains where weekly google trends data can only be downloaded 5 years back"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "O8BKAHH8Vbme",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b8c1606-05bd-423c-fdd4-8a88fd532a44"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Median Annualized Return': np.float64(0.4279),\n",
              " 'Median Annualized Volatility': np.float64(0.3414),\n",
              " 'Median Sharpe Ratio': np.float64(1.3252)}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Set random seed\n",
        "random.seed(10)\n",
        "\n",
        "merged = pd.read_csv(\"merged.csv\", index_col=0, parse_dates=True)\n",
        "\n",
        "# Load data\n",
        "df = merged\n",
        "\n",
        "# Split into training and testing\n",
        "train = df[df.index < \"2024-01-01\"]\n",
        "test = df[df.index >= \"2024-01-01\"]\n",
        "\n",
        "# Separate returns and risk-free rate\n",
        "train_rf = train['risk_free_rate_weekly']\n",
        "test_rf = test['risk_free_rate_weekly']\n",
        "\n",
        "# Remove non-numeric and risk-free column from returns\n",
        "returns_columns = [col for col in df.columns if col != \"risk_free_rate_weekly\"]\n",
        "\n",
        "# Storage for metrics\n",
        "all_returns = []\n",
        "all_stds = []\n",
        "all_sharpes = []\n",
        "\n",
        "# Simulation loop\n",
        "for _ in range(1000):\n",
        "    selected_assets = random.sample(returns_columns, k=random.randint(6, 10))\n",
        "\n",
        "    train_returns = train[selected_assets].replace([np.inf, -np.inf], np.nan).dropna()\n",
        "    test_returns = test[selected_assets].replace([np.inf, -np.inf], np.nan)\n",
        "    test_returns = test_returns[train_returns.columns]\n",
        "\n",
        "    if train_returns.empty or test_returns.empty:\n",
        "        continue\n",
        "\n",
        "    mu = train_returns.mean() * 52\n",
        "    cov = train_returns.cov() * 52\n",
        "    rf = train_rf.mean() * 52\n",
        "\n",
        "    n = len(mu)\n",
        "    x0 = np.array([1/n] * n)\n",
        "    bounds = tuple((0, 1) for _ in range(n))  # No short-selling\n",
        "    constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}  # Must sum to 1\n",
        "\n",
        "    try:\n",
        "        result = minimize(\n",
        "            lambda w: -((np.dot(w, mu) - rf) / (np.sqrt(np.dot(w.T, np.dot(cov, w))))),\n",
        "            x0, method='SLSQP', bounds=bounds, constraints=constraints\n",
        "        )\n",
        "\n",
        "        if result.success:\n",
        "            weights = result.x\n",
        "            test_portfolio_returns = test_returns.dot(weights)\n",
        "            test_excess_returns = test_portfolio_returns - test_rf.loc[test_returns.index].values\n",
        "\n",
        "            ann_return = (1 + test_excess_returns.mean()) ** 52 - 1\n",
        "            ann_std = test_excess_returns.std() * np.sqrt(52)\n",
        "            sharpe = ann_return / ann_std\n",
        "\n",
        "            all_returns.append(ann_return)\n",
        "            all_stds.append(ann_std)\n",
        "            all_sharpes.append(sharpe)\n",
        "\n",
        "    except Exception:\n",
        "        continue\n",
        "\n",
        "# Compute medians\n",
        "median_return = np.median(all_returns)\n",
        "median_std = np.median(all_stds)\n",
        "median_sharpe = np.median(all_sharpes)\n",
        "\n",
        "{\n",
        "    \"Median Annualized Return\": np.round(median_return, 4),\n",
        "    \"Median Annualized Volatility\": np.round(median_std, 4),\n",
        "    \"Median Sharpe Ratio\": np.round(median_sharpe, 4)\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZgoll2rarxi"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GRU, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from scipy.optimize import minimize\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "df = merged\n",
        "\n",
        "# Use only return columns\n",
        "returns_columns = [col for col in df.columns if col != \"risk_free_rate_weekly\"]\n",
        "\n",
        "# Split into training and test sets\n",
        "train_df = df[df.index < \"2024-01-01\"]\n",
        "test_df = df[df.index >= \"2024-01-01\"]\n",
        "test_rf = test_df['risk_free_rate_weekly']\n",
        "\n",
        "# Hyperparameter grid for tuning\n",
        "param_grid = {\n",
        "    'units': [16, 32],\n",
        "    'dropout': [0.1, 0.2],\n",
        "    'recurrent_dropout': [0.1],\n",
        "    'batch_size': [8],\n",
        "    'epochs': [30],\n",
        "    'lr': [0.001, 0.0005],\n",
        "    'seq_len': [20]\n",
        "}\n",
        "\n",
        "def create_sequences(data, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i + seq_length])\n",
        "        y.append(data[i + seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Tuning\n",
        "returns_columns = [col for col in train_df.columns if col != 'risk_free_rate_weekly']\n",
        "test_rf = test_df['risk_free_rate_weekly']\n",
        "best_sharpe = -np.inf\n",
        "best_params = None\n",
        "\n",
        "for params in ParameterGrid(param_grid):\n",
        "    predicted_returns = {}\n",
        "    valid_assets = []\n",
        "\n",
        "    for asset in returns_columns:\n",
        "        series = train_df[asset].dropna().values.reshape(-1, 1)\n",
        "        if len(series) < 60:\n",
        "            continue\n",
        "\n",
        "        # Smooth + scale\n",
        "        series = pd.Series(series.flatten()).rolling(3, min_periods=1).mean().values.reshape(-1, 1)\n",
        "        scaler = MinMaxScaler()\n",
        "        scaled_series = scaler.fit_transform(series)\n",
        "\n",
        "        X, y = create_sequences(scaled_series, params['seq_len'])\n",
        "        if len(X) == 0:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Model\n",
        "            model = Sequential([\n",
        "                GRU(params['units'], input_shape=(params['seq_len'], 1),\n",
        "                    dropout=params['dropout'], recurrent_dropout=params['recurrent_dropout']),\n",
        "                Dense(1)\n",
        "            ])\n",
        "            model.compile(optimizer=Adam(learning_rate=params['lr']), loss='mse')\n",
        "            model.fit(X, y, epochs=params['epochs'], batch_size=params['batch_size'], verbose=0,\n",
        "                      callbacks=[EarlyStopping(patience=5, restore_best_weights=True)])\n",
        "\n",
        "            # Forecast 52 steps\n",
        "            last_seq = scaled_series[-params['seq_len']:]\n",
        "            preds = []\n",
        "            for _ in range(52):\n",
        "                input_seq = last_seq.reshape(1, params['seq_len'], 1)\n",
        "                pred = model.predict(input_seq, verbose=0)\n",
        "                preds.append(pred[0][0])\n",
        "                last_seq = np.append(last_seq[1:], pred).reshape(params['seq_len'], 1)\n",
        "\n",
        "            preds = scaler.inverse_transform(np.array(preds).reshape(-1, 1)).flatten()\n",
        "            predicted_returns[asset] = preds\n",
        "            valid_assets.append(asset)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    # Optimize portfolio if enough assets\n",
        "    if len(valid_assets) < 6:\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        mu = pd.DataFrame(predicted_returns).mean().values * 52\n",
        "        cov = train_df[valid_assets].cov().values * 52\n",
        "        n = len(valid_assets)\n",
        "        x0 = np.array([1/n] * n)\n",
        "        bounds = tuple((0, 1) for _ in range(n))\n",
        "        constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}\n",
        "\n",
        "        result = minimize(\n",
        "            lambda w: -((np.dot(w, mu) - test_rf.mean() * 52) /\n",
        "                        np.sqrt(np.dot(w.T, np.dot(cov, w)))),\n",
        "            x0, method='SLSQP', bounds=bounds, constraints=constraints\n",
        "        )\n",
        "\n",
        "        if result.success:\n",
        "            weights = result.x\n",
        "            test_returns = test_df[valid_assets].dot(weights)\n",
        "            excess = test_returns - test_rf.loc[test_returns.index].values\n",
        "            ann_return = (1 + excess.mean()) ** 52 - 1\n",
        "            ann_std = excess.std() * np.sqrt(52)\n",
        "            sharpe = ann_return / ann_std\n",
        "\n",
        "            if sharpe > best_sharpe:\n",
        "                best_sharpe = sharpe\n",
        "                best_params = params\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "print(\"Best GRU Hyperparameters:\", best_params)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6fPKUW4qCto"
      },
      "source": [
        "Best GRU Hyperparameters: {'batch_size': 8, 'dropout': 0.1, 'epochs': 30, 'lr': 0.001, 'recurrent_dropout': 0.1, 'seq_len': 20, 'units': 32}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZFqt9RczSSB"
      },
      "outputs": [],
      "source": [
        "# GRU-based Simulation with Best Hyperparameters\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GRU, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from scipy.optimize import minimize\n",
        "import random\n",
        "\n",
        "df = merged\n",
        "\n",
        "# Column setup\n",
        "returns_columns = [col for col in df.columns if col != \"risk_free_rate_weekly\"]\n",
        "train_df = df[df.index < \"2024-01-01\"]\n",
        "test_df = df[df.index >= \"2024-01-01\"]\n",
        "test_rf = test_df['risk_free_rate_weekly']\n",
        "\n",
        "# Helper to create GRU sequences\n",
        "def create_sequences(data, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i + seq_length])\n",
        "        y.append(data[i + seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# GRU hyperparameters\n",
        "GRU_PARAMS = {\n",
        "    'units': 32,\n",
        "    'dropout': 0.1,\n",
        "    'recurrent_dropout': 0.1,\n",
        "    'batch_size': 8,\n",
        "    'epochs': 30,\n",
        "    'lr': 0.001,\n",
        "    'seq_len': 20\n",
        "}\n",
        "\n",
        "# Run 109 simulations\n",
        "all_returns, all_stds, all_sharpes = [], [], []\n",
        "random.seed(10)\n",
        "\n",
        "for _ in range(10):\n",
        "    selected_assets = random.sample(returns_columns, k=random.randint(6, 10))\n",
        "    predicted_returns = {}\n",
        "    valid_assets = []\n",
        "\n",
        "    for asset in selected_assets:\n",
        "        series = train_df[asset].dropna().values.reshape(-1, 1)\n",
        "        if len(series) < 60:\n",
        "            continue\n",
        "\n",
        "        # Smooth and scale\n",
        "        series = pd.Series(series.flatten()).rolling(window=3, min_periods=1).mean().values.reshape(-1, 1)\n",
        "        scaler = MinMaxScaler()\n",
        "        scaled_series = scaler.fit_transform(series)\n",
        "\n",
        "        # Create sequences\n",
        "        X, y = create_sequences(scaled_series, GRU_PARAMS['seq_len'])\n",
        "\n",
        "        model = Sequential([\n",
        "            GRU(GRU_PARAMS['units'],\n",
        "                input_shape=(X.shape[1], 1),\n",
        "                dropout=GRU_PARAMS['dropout'],\n",
        "                recurrent_dropout=GRU_PARAMS['recurrent_dropout']),\n",
        "            Dense(1)\n",
        "        ])\n",
        "        model.compile(optimizer=Adam(learning_rate=GRU_PARAMS['lr']), loss='mse')\n",
        "        model.fit(X, y, epochs=GRU_PARAMS['epochs'], batch_size=GRU_PARAMS['batch_size'],\n",
        "                  verbose=0, callbacks=[EarlyStopping(patience=5, restore_best_weights=True)])\n",
        "\n",
        "        # Forecast next 52 returns\n",
        "        last_seq = scaled_series[-GRU_PARAMS['seq_len']:]\n",
        "        preds = []\n",
        "        for _ in range(52):\n",
        "            input_seq = last_seq.reshape(1, GRU_PARAMS['seq_len'], 1)\n",
        "            pred = model.predict(input_seq, verbose=0)[0][0]\n",
        "            preds.append(pred)\n",
        "            last_seq = np.append(last_seq[1:], [[pred]], axis=0)\n",
        "\n",
        "        preds = scaler.inverse_transform(np.array(preds).reshape(-1, 1)).flatten()\n",
        "        predicted_returns[asset] = preds\n",
        "        valid_assets.append(asset)\n",
        "\n",
        "    if len(valid_assets) < 6:\n",
        "        continue\n",
        "\n",
        "    mu = pd.DataFrame(predicted_returns).mean().values * 52\n",
        "    cov = train_df[valid_assets].cov().values * 52\n",
        "    rf = train_df['risk_free_rate_weekly'].mean() * 52\n",
        "\n",
        "    n = len(valid_assets)\n",
        "    x0 = np.array([1/n] * n)\n",
        "    bounds = tuple((0, 1) for _ in range(n))\n",
        "    constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}\n",
        "\n",
        "    try:\n",
        "        result = minimize(\n",
        "            lambda w: -((np.dot(w, mu) - rf) / np.sqrt(np.dot(w.T, np.dot(cov, w)))),\n",
        "            x0, method='SLSQP', bounds=bounds, constraints=constraints\n",
        "        )\n",
        "\n",
        "        if result.success:\n",
        "            weights = result.x\n",
        "            test_returns = test_df[valid_assets].dot(weights)\n",
        "            test_excess_returns = test_returns - test_rf.loc[test_returns.index].values\n",
        "\n",
        "            ann_return = (1 + test_excess_returns.mean()) ** 52 - 1\n",
        "            ann_std = test_excess_returns.std() * np.sqrt(52)\n",
        "            sharpe = ann_return / ann_std\n",
        "\n",
        "            all_returns.append(ann_return)\n",
        "            all_stds.append(ann_std)\n",
        "            all_sharpes.append(sharpe)\n",
        "\n",
        "    except Exception:\n",
        "        continue\n",
        "\n",
        "# Final output\n",
        "print({\n",
        "    \"Median Annualized Return\": np.round(np.median(all_returns), 4),\n",
        "    \"Median Annualized Volatility\": np.round(np.median(all_stds), 4),\n",
        "    \"Median Sharpe Ratio\": np.round(np.median(all_sharpes), 4)\n",
        "})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaUkJJAnbLz9"
      },
      "source": [
        "SP500 Results to compare as benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RutUbgfVnBFb"
      },
      "outputs": [],
      "source": [
        "import yfinance as yf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Download S&P 500 data for 2024\n",
        "sp500 = yf.download('^GSPC', start='2024-01-01', end='2024-12-31', progress=False)\n",
        "\n",
        "# Calculate daily returns\n",
        "sp500['Daily Return'] = sp500['Close'].pct_change()\n",
        "\n",
        "# Annualized return\n",
        "daily_returns = sp500['Daily Return'].dropna()\n",
        "annualized_return = ((1 + daily_returns.mean()) ** 252 - 1)\n",
        "\n",
        "# Annualized volatility\n",
        "annualized_volatility = daily_returns.std() * np.sqrt(252)\n",
        "\n",
        "# Sharpe ratio\n",
        "risk_free_rate = 0.045\n",
        "sharpe_ratio = (annualized_return - risk_free_rate) / annualized_volatility\n",
        "\n",
        "# Print results\n",
        "print(f\"Annualized Return: {annualized_return:.4%}\")\n",
        "print(f\"Annualized Volatility: {annualized_volatility:.4%}\")\n",
        "print(f\"Sharpe Ratio: {sharpe_ratio:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtx0GH7gT7Sj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.optimize import minimize\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load data\n",
        "df_alt = pd.read_csv(\"df_alt.csv\", index_col=\"Date\", parse_dates=True)\n",
        "\n",
        "# Columns\n",
        "returns_columns = [\n",
        "    'ADBE', 'AVGO', 'BRK-B', 'COST', 'DECK', 'HD', 'HUBB', 'INTC', 'JNJ',\n",
        "    'KO', 'MANH', 'MRK', 'NDSN', 'PEP', 'PG', 'POOL', 'RPM', 'SMCI', 'STLD', 'WLK'\n",
        "]\n",
        "macro_columns = ['FedFundsRate', 'CPI', 'WeeklyInflationRate', 'UnemploymentRate']\n",
        "trend_columns = [f\"{ticker}_trend\" for ticker in returns_columns]\n",
        "\n",
        "# Train/test split\n",
        "train_df = df_alt[df_alt.index < \"2024-01-01\"]\n",
        "test_df = df_alt[df_alt.index >= \"2024-01-01\"]\n",
        "\n",
        "# Hyperparameter distributions\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 150, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['sqrt', 'log2']\n",
        "}\n",
        "\n",
        "best_params = {}  # Store the best parameters for each asset\n",
        "\n",
        "for asset in returns_columns:\n",
        "    # Select features for this asset\n",
        "    cols = [asset, f\"{asset}_trend\"] + macro_columns\n",
        "    df_feat = train_df[cols].copy()\n",
        "    df_feat[\"target\"] = df_feat[asset].shift(-1)  # Predicting next period's return\n",
        "    df_feat.dropna(inplace=True)  # Remove rows with missing values\n",
        "\n",
        "    if len(df_feat) < 80:\n",
        "        continue\n",
        "\n",
        "    X = df_feat.drop(columns=\"target\").values\n",
        "    y = df_feat[\"target\"].values\n",
        "    scaler = StandardScaler()  # Scale features\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    model = RandomForestRegressor(random_state=10)\n",
        "    # Hyperparameter tuning with cross-validation\n",
        "    search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=10, cv=3, n_jobs=-1, random_state=10)\n",
        "    search.fit(X_scaled, y)\n",
        "    best_params[asset] = search.best_params_\n",
        "\n",
        "# Best parameters for each asset\n",
        "print(\"\\n Best Parameters Used:\")\n",
        "for asset, params in best_params.items():\n",
        "    print(f\"{asset}: {params}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Portfolio optimzation with RF\n"
      ],
      "metadata": {
        "id": "e2rVAiQp-UyW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqzM3CgAUGQQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.optimize import minimize\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load dataset\n",
        "df_alt = pd.read_csv(\"/content/df_alt.csv\", index_col=\"Date\", parse_dates=True)\n",
        "\n",
        "# Define columns\n",
        "returns_columns = [\n",
        "    'ADBE', 'AVGO', 'BRK-B', 'COST', 'DECK', 'HD', 'HUBB', 'INTC', 'JNJ',\n",
        "    'KO', 'MANH', 'MRK', 'NDSN', 'PEP', 'PG', 'POOL', 'RPM', 'SMCI', 'STLD', 'WLK'\n",
        "]\n",
        "macro_columns = ['FedFundsRate', 'CPI', 'WeeklyInflationRate', 'UnemploymentRate']\n",
        "trend_columns = [f\"{ticker}_trend\" for ticker in returns_columns]\n",
        "\n",
        "# Split data\n",
        "train_df = df_alt[df_alt.index < \"2024-01-01\"]\n",
        "test_df = df_alt[df_alt.index >= \"2024-01-01\"]\n",
        "test_rf = test_df[\"risk_free_rate_weekly\"]\n",
        "\n",
        "# Hyperparameter tuning\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 150, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['sqrt', 'log2']\n",
        "}\n",
        "best_params = {}\n",
        "\n",
        "for asset in returns_columns:\n",
        "    cols = [asset, f\"{asset}_trend\"] + macro_columns\n",
        "    df_feat = train_df[cols].copy()\n",
        "    df_feat[\"target\"] = df_feat[asset].shift(-1)\n",
        "    df_feat.dropna(inplace=True)\n",
        "\n",
        "    if len(df_feat) < 80:\n",
        "        continue\n",
        "\n",
        "    X = df_feat.drop(columns=\"target\").values\n",
        "    y = df_feat[\"target\"].values\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    model = RandomForestRegressor(random_state=10)\n",
        "    search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=10, cv=3, n_jobs=-1, random_state=10)\n",
        "    search.fit(X_scaled, y)\n",
        "    best_params[asset] = search.best_params_\n",
        "\n",
        "# Forecasting and optimization\n",
        "def clip_predictions(preds, lower=-0.01, upper=0.01):\n",
        "    return np.clip(preds, lower, upper)\n",
        "\n",
        "all_returns, all_stds, all_sharpes = [], [], []\n",
        "random.seed(10)\n",
        "\n",
        "for _ in range(100):\n",
        "    selected_assets = random.sample(list(best_params.keys()), k=random.randint(6, 10))\n",
        "    predicted_returns = {}\n",
        "    valid_assets = []\n",
        "\n",
        "    for asset in selected_assets:\n",
        "        cols = [asset, f\"{asset}_trend\"] + macro_columns\n",
        "        df_feat = train_df[cols].copy()\n",
        "        df_feat[\"target\"] = df_feat[asset].shift(-1)\n",
        "        df_feat.dropna(inplace=True)\n",
        "\n",
        "        if len(df_feat) < 80:\n",
        "            continue\n",
        "\n",
        "        X = df_feat.drop(columns=\"target\").values\n",
        "        y = df_feat[\"target\"].values\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "        model = RandomForestRegressor(**best_params[asset], random_state=10, max_samples=0.8)\n",
        "        model.fit(X_scaled, y)\n",
        "\n",
        "        test_features = test_df[cols].dropna()\n",
        "        if len(test_features) < len(test_df):\n",
        "            continue\n",
        "\n",
        "        X_test_scaled = scaler.transform(test_features.values)\n",
        "        preds = clip_predictions(model.predict(X_test_scaled))\n",
        "\n",
        "        if len(preds) >= len(test_df):\n",
        "            predicted_returns[asset] = preds[:len(test_df)]\n",
        "            valid_assets.append(asset)\n",
        "\n",
        "    if len(valid_assets) < 6:\n",
        "        continue\n",
        "\n",
        "    mu = pd.DataFrame(predicted_returns).mean().values * 52\n",
        "    cov = train_df[valid_assets].cov().values * 52\n",
        "\n",
        "    n = len(mu)\n",
        "    x0 = np.array([1 / n] * n)\n",
        "    bounds = tuple((0, 1) for _ in range(n))\n",
        "    constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}\n",
        "\n",
        "    try:\n",
        "        result = minimize(\n",
        "            lambda w: -((np.dot(w, mu) - test_rf.mean() * 52) / np.sqrt(np.dot(w.T, np.dot(cov, w)))),\n",
        "            x0, method='SLSQP', bounds=bounds, constraints=constraints\n",
        "        )\n",
        "\n",
        "        if result.success:\n",
        "            weights = result.x\n",
        "            test_returns = test_df[valid_assets].dot(weights)\n",
        "            test_excess_returns = test_returns - test_rf.loc[test_returns.index].values\n",
        "\n",
        "            ann_return = (1 + test_excess_returns.mean()) ** 52 - 1\n",
        "            ann_std = test_excess_returns.std() * np.sqrt(52)\n",
        "            sharpe = ann_return / ann_std\n",
        "\n",
        "            all_returns.append(ann_return)\n",
        "            all_stds.append(ann_std)\n",
        "            all_sharpes.append(sharpe)\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "results = {\n",
        "    \"Median Annualized Return\": np.round(np.median(all_returns), 4),\n",
        "    \"Median Annualized Volatility\": np.round(np.median(all_stds), 4),\n",
        "    \"Median Sharpe Ratio\": np.round(np.median(all_sharpes), 4)\n",
        "}\n",
        "\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Portfolio Optimzation with XGB"
      ],
      "metadata": {
        "id": "W9Frwq3b-ZNL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IssE2bCIUKTX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from xgboost import XGBRegressor\n",
        "from scipy.optimize import minimize\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load dataset\n",
        "df_alt = pd.read_csv(\"/content/df_alt.csv\", index_col=\"Date\", parse_dates=True)\n",
        "\n",
        "# Define columns\n",
        "returns_columns = [\n",
        "    'ADBE', 'AVGO', 'BRK-B', 'COST', 'DECK', 'HD', 'HUBB', 'INTC', 'JNJ',\n",
        "    'KO', 'MANH', 'MRK', 'NDSN', 'PEP', 'PG', 'POOL', 'RPM', 'SMCI', 'STLD', 'WLK'\n",
        "]\n",
        "macro_columns = ['FedFundsRate', 'CPI', 'WeeklyInflationRate', 'UnemploymentRate']\n",
        "\n",
        "# Split data\n",
        "train_df = df_alt[df_alt.index < \"2024-01-01\"]\n",
        "test_df = df_alt[df_alt.index >= \"2024-01-01\"]\n",
        "test_rf = test_df[\"risk_free_rate_weekly\"]\n",
        "\n",
        "# Hyperparameter tuning for XGBoost\n",
        "param_dist = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [2, 3, 4, 5],\n",
        "    'learning_rate': [0.01, 0.05, 0.1]\n",
        "}\n",
        "best_params = {}\n",
        "\n",
        "for asset in returns_columns:\n",
        "    cols = [asset, f\"{asset}_trend\"] + macro_columns\n",
        "    df_feat = train_df[cols].copy()\n",
        "    df_feat[\"target\"] = df_feat[asset].shift(-1)\n",
        "    df_feat.dropna(inplace=True)\n",
        "\n",
        "    if len(df_feat) < 80:\n",
        "        continue\n",
        "\n",
        "    X = df_feat.drop(columns=\"target\").values\n",
        "    y = df_feat[\"target\"].values\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    model = XGBRegressor(random_state=10)\n",
        "    search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=10, cv=3, n_jobs=-1, random_state=10)\n",
        "    search.fit(X_scaled, y)\n",
        "    best_params[asset] = search.best_params_\n",
        "\n",
        "# Forecasting and optimization\n",
        "def clip_predictions(preds, lower=-0.01, upper=0.01):\n",
        "    return np.clip(preds, lower, upper)\n",
        "\n",
        "all_returns, all_stds, all_sharpes = [], [], []\n",
        "random.seed(10)\n",
        "\n",
        "for _ in range(100):\n",
        "    selected_assets = random.sample(list(best_params.keys()), k=random.randint(6, 10))\n",
        "    predicted_returns = {}\n",
        "    valid_assets = []\n",
        "\n",
        "    for asset in selected_assets:\n",
        "        cols = [asset, f\"{asset}_trend\"] + macro_columns\n",
        "        df_feat = train_df[cols].copy()\n",
        "        df_feat[\"target\"] = df_feat[asset].shift(-1)\n",
        "        df_feat.dropna(inplace=True)\n",
        "\n",
        "        if len(df_feat) < 80:\n",
        "            continue\n",
        "\n",
        "        X = df_feat.drop(columns=\"target\").values\n",
        "        y = df_feat[\"target\"].values\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "        model = XGBRegressor(**best_params[asset], random_state=10)\n",
        "        model.fit(X_scaled, y)\n",
        "\n",
        "        test_features = test_df[cols].dropna()\n",
        "        if len(test_features) < len(test_df):\n",
        "            continue\n",
        "\n",
        "        X_test_scaled = scaler.transform(test_features.values)\n",
        "        preds = clip_predictions(model.predict(X_test_scaled))\n",
        "\n",
        "        if len(preds) >= len(test_df):\n",
        "            predicted_returns[asset] = preds[:len(test_df)]\n",
        "            valid_assets.append(asset)\n",
        "\n",
        "    if len(valid_assets) < 6:\n",
        "        continue\n",
        "\n",
        "    mu = pd.DataFrame(predicted_returns).mean().values * 52\n",
        "    cov = train_df[valid_assets].cov().values * 52\n",
        "\n",
        "    n = len(mu)\n",
        "    x0 = np.array([1 / n] * n)\n",
        "    bounds = tuple((0, 1) for _ in range(n))\n",
        "    constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}\n",
        "\n",
        "    try:\n",
        "        result = minimize(\n",
        "            lambda w: -((np.dot(w, mu) - test_rf.mean() * 52) / np.sqrt(np.dot(w.T, np.dot(cov, w)))),\n",
        "            x0, method='SLSQP', bounds=bounds, constraints=constraints\n",
        "        )\n",
        "\n",
        "        if result.success:\n",
        "            weights = result.x\n",
        "            test_returns = test_df[valid_assets].dot(weights)\n",
        "            test_excess_returns = test_returns - test_rf.loc[test_returns.index].values\n",
        "\n",
        "            ann_return = test_excess_returns.mean() * 52\n",
        "            ann_std = test_excess_returns.std() * np.sqrt(52)\n",
        "            sharpe = ann_return / ann_std\n",
        "\n",
        "            all_returns.append(ann_return)\n",
        "            all_stds.append(ann_std)\n",
        "            all_sharpes.append(sharpe)\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "results = {\n",
        "    \"Median Annualized Return\": np.round(np.median(all_returns), 4),\n",
        "    \"Median Annualized Volatility\": np.round(np.median(all_stds), 4),\n",
        "    \"Median Sharpe Ratio\": np.round(np.median(all_sharpes), 4)\n",
        "}\n",
        "\n",
        "results"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
